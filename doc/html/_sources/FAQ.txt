=====
 FAQ
=====


.. contents::
   :local:
   :backlinks: top

Mount fails with `failed to open /dev/fuse`
===========================================

You should make sure that you have the rights to mount FUSE file
systems. In Debian/Ubuntu, you have to add yourself to the *fuse*
group (e.g. with ``adduser <myself> fuse`` executed as root).

If you are running under OpenVZ, you also have to explicitly give your
host VM permissions to access the ``/dev/fuse`` device (otherwise you
will get the above message even if ``/dev/fuse`` is world-writable,
thanks to waltherlalk for the solution).

Which operating systems are supported?
======================================

S3QL is developed on Linux, but should in principle work on all FUSE
supported operating systems that have the required Python modules. The
`umount.s3ql` program does some tricks which may not work on non-Linux
systems, but you can always umount with the `fusermount -u` command
provided by FUSE itself (the only difference is that it will not block
until all data has been uploaded to S3 but return immediately). Please
report your success (or failure) with other operating systems on the
mailing list, so that this information can be extended.

=== Writing data seems to be very slow on my system... ===

Probably you are using a FUSE version that does not support "big writes". Make sure that you have at least FUSE 2.8 and kernel 2.6.26. FUSE 2.8.1 packages for Ubuntu Karmic are available on the [http://code.google.com/p/s3ql/downloads/list downloads page].

If you already using FUSE 2.8.1, installing [http://psyco.sourceforge.net/ Psyco] (Debian/Ubuntu package `python-psyco`) may give you a little performance boost (S3QL will use Psyco automatically if it is installed).

=== How much do I have to pay Amazon for storage in S3? ===

Amazon's current pricing is described on [http://aws.amazon.com/s3/#pricing http://aws.amazon.com/s3/#pricing]. There is also a [http://calculator.s3.amazonaws.com/calc5.html pricing calculator].

The rough costs that occur when using a bucket to store an S3QL file system are explained on the [S3PricingCompatibility  Interplay with Amazon's S3 Pricing] page.

=== Is there a file size limit? ===

No, S3QL puts no limits on the size of your files. The 5 GB limit that Amazon imposes on the limit of individual S3 objects does not matter for S3QL, since files are automatically and transparently split into smaller blocks.

=== Suppose I want to make a small change in a very large file. Will S3QL download and re-upload the entire file? ===

No, S3QL splits files in blocks of a configurable size (default: 10 MB). So to make a small change in a big file, only one block needs to be transferred and not the entire file.

=== I don't quite understand this de-duplication feature... ===

The great thing about data de-duplication is that you don't need to know anything about it and will still get all the benefits. Towards you, S3QL will behave and look exactly like a file system without data duplication. When talking to the storage backend, however, the S3QL will try to save duplicated data only once and thus take less storage than an ordinary file system would need.

=== What's wrong with having S3 buckets in the "US Standard" region? ===

The problem with the "US Standard" bucket location is that Amazon gives weaker consistency guarantees for data stored in this location. If S3QL has written a new object into a US-Standard bucket, and afterwards _all_ the Amazon servers that have copies of the new object happen to crash, then an attempt to read the object will fail because the object appears not to exist.

S3QL detects and handles this situation keeping track of the objects that it has uploaded, so even the above constellation will generally not lead to problems.

However, the list of uploaded objects itself needs to be stored in S3 as well, and it is therefore theoretically possibly to construct the following race condition:

  # An S3QL file system is mounted, some data is modified
  # The file system is unmounted, the new list of active S3 objects is uploaded
  # All S3 servers that hold copies of this list crash
  # The S3QL file system is mounted again. S3QL downloads the most recent list of active objects
  # Since the list that has been uploaded in (2) is not visible (because the servers storing it are still down), S3QL downloads an outdated list and weird file system errors  occur

Obviously this is a *very* remote chance, since it requires that _all_ the servers holding a copy of one specific object to be down just at the time when you mount the file system (if they go down _while_ the file system is mounted this will not cause any problems). However, it is nevertheless a possibility and therefore S3QL recommends to store buckets in the N. California and EU storage regions, where this problem cannot occur at all. (Once an object has been written into a N. California or EU bucket, Amazon guarantees that it will stay available under all circumstances).

=== What blocksize should I use? ===

A reasonable blocksize is around 1 MB to 50 MB, with 10 MB the default.

If you are going to make lots of small changes to big files, it might be worth to try a lower blocksize to get improved performance (but you should really test this before you go for it).

Generally you should not need to deviate from the default blocksize, unless you know that you have a good reason to do so.

=== I don't want to enter my bucket password every time... ===

You can save your password in a file, say `/home/ron/bucket_password` and then pipe the password directly into mount.s3ql, i.e. you execute `cat /home/ron/bucket_password | mount.s3ql <options>`.
