.. -*- mode: rst -*-

================
Managing Backups
================

S3QL ships with a program called `expire_backups` that can be used to
manage backups according to different backup strategies. When combined
with `rsync <http://samba.org/rsync>`_, this allows to easily
implement a simple but powerful solution to store backups on S3QL file
systems.

The basic tasks that have to be done for a backup are:

#. Mount the file system
#. Replicate the previous backup with `s3qlcp`
#. Update the new copy with the data from the backup source using rsync
#. Delete old backups that are no longer needed
#. Unmount the file system


Before we present a simple shell script that automates these scripts,
you should get familiar with the `expire_backup` command.

The `expire_backups` command
============================

The `expire_backups` command deletes backups that are no longer needed
as defined by a specific backup strategy. A backup strategy is defined
by specifying a number of backup *generations* with their relative
ages.

Let's illustrate this with an example. A very common idea of a backup
strategy is to keep backups such that there is always at least one
backup from given number of points in the past. Naively, you could
formalize this by saying that you always want to have backups
available from the following dates:

* Yesterday
* 3 days ago
* A week ago
* A month ago
* 6 months ago

The first thing that you should realize is that this is actually most
likely *not* what you really want, because it would require you to do
daily backups and keep *every* backup for the full 6 months (otherwise
you wouldn't always have a backup that has *exactly* the requested
age). So what you probably mean by the above is that you would like to
have backups that are from different date *ranges*, like this:

* 1 hour to 24 hours ago
* 24 hours to 3 days ago
* 3 days to 7 days ago
* 7 days to 31 days ago
* 31 days to 6*31 ~ 180 days ago

However, this still is not perfect. Imagine that you go on vacation
for three weeks and don't do any backups. When you come back and do a
fresh backup, a program following your prescription would delete the
backups that you did right before your vacation because they're now 3
weeks old. So if later that day you want to recover a document that
you deleted one day before going on vacation, you'd be in for a big
disappointment because that backup has already been deleted.

The problem is that logically the backup that you did right before
your vacation is only 1 day old: since you did not change any data
while on vacation, this period can not be taken into account for
calculating backup ages. A really good backup strategy therefore only
considers *relative* ages between backup *generations*, e.g.

* Generation one: backups made 1 hour to 24 hours before the
  current backup
* Generation two: backups made 24 hours to 2 days before the first
  backup of generation one
* Generation three: backups made 2 days to 5 days before the first
  backup of generation two
* Generation four: backups made * 5 days to 26 days before the
  first backup of generation three
* Generation five: backups made * 26 days to 154 days before the
  first backup of generation four

The `expire_backups` command allows you to specify your backup
strategy in terms of such generations. It uses a sophisticated
algorithm that ensures that there will always be at least one backup
available in each generation. It deletes backups are deleted as soon
as they will no longer be required for any generation at any time
(both now and in the future).

`expire_backups` usage is simple. It requires backups to have names of
the forms `year-month-day_hour:minute:seconds` (`YYYY-MM-DD_HH:mm:ss`)
and works on all backups in the current directory. The desired backup
generations are specified by listing their relative ages. So for the
above backup strategy, the correct invocation would be::

  expire_backups 1h 24h 2d 5d 26d 154d



A sample backup script
======================

The following scripts automates the steps described at the beginning
of this page. It mounts the file system, figures out the most recent
backup and replicates it, updates the copy, calls `expire_backups` to
get rid of unneeded old backups and umounts the file system. The
backups are stored in directories of the form `YYYY-MM-DD_HH:mm:SS` as
required by `expire_backups` ::

  #!/bin/bash

  # Abort entire script if any command fails
  set -e

  # Backup destination  (storage url)
  bucket="s3://my_backup_bucket"

  # Recover cache if e.g. system was shut down while fs was mounted
  fsck.s3ql --batch "$bucket"

  # Create a temporary mountpoint and mount file system
  mountpoint="/tmp/s3ql_backup_$$"
  mkdir "$mountpoint"
  mount.s3ql "$bucket" "$mountpoint"

  # Make sure the file system is unmounted when we are done
  trap "cd /; umount.s3ql '$mountpoint'; rmdir '$mountpoint'" EXIT

  # Figure out the most recent backup
  cd "$mountpoint"
  last_backup=`python <<EOF
  import os
  import re
  backups=sorted(x for x in os.listdir('.') if re.match(r'^[\\d-]{10}_[\\d:]{8}$', x))
  if backups:
      print backups[-1]
  EOF`

  # Duplicate the most recent backup unless this is the first backup
  new_backup=`date "+%Y-%m-%d_%H:%M:%S"`
  if [ -n "$last_backup" ]; then
      echo "Copying $last_backup to $new_backup..."
      s3qlcp "$last_backup" "$new_backup"
  fi

  # ..and update the copy
  rsync -aHAXx --delete-during --delete-excluded --partial -v \
      --exclude /.cache/ \
      --exclude /.s3ql/ \
      --exclude /.thumbnails/ \
      --exclude /tmp/ \
      "/home/my_username/" "./$new_backup/"

  # Expire old backups, keep 6 generations with specified relative ages
  expire_backups 9h 1d 7d 14d 31d 150d 150d
