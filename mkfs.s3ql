#!/usr/bin/env python
#
#    Copyright (C) 2008  Nikolaus Rath <Nikolaus@rath.org>
#
#    This program can be distributed under the terms of the GNU LGPL.
#

from boto.s3.connection import S3Connection
from boto.exception import *
from boto.s3.key import Key
import apsw
import sys
import os
import hashlib
import stat
from getpass import getpass
import shutil
from time import time, sleep
from string import Template
from optparse import OptionParser
import s3ql


def main():
    """Initializes an S3QL filesystem.
    """

    #
    # Parse options
    #
    parser = OptionParser(
        usage="%prog  [options] <bucketname>\n" \
            "       %prog --help",
        description="Initializes on Amazon S3 bucket for use as a filesystem")

    parser.add_option("--awskey", type="string",
                      help="Amazon Webservices access key to use")
    parser.add_option("-L", type="string", help="Filesystem label",
                      dest="name")
    parser.add_option("--blocksize", type="int", default=10240,
                      help="Maximum size of s3 objects in KB (default: %default)")
    parser.add_option("-f", action="store_true", default=False,
                      dest="force", help="Force creation and remove any existing data")
    parser.add_option("-e", action="store_true", default=False,
                      dest="encrypt", help="Create an AES encrypted filesystem")

    (options, pps) = parser.parse_args()

    if not len(pps) == 1:
        parser.error("bucketname not specificed")
    bucket = pps[0]

    (awskey, awspass) = s3ql.get_credentials(options.awskey)

    if options.encrypt:
        if sys.stdin.isatty():
            options.encrypt = getpass("Enter encryption password: ")
            if not options.encrypt == getpass("Confirm encryption password: "):
                print >>sys.stderr, "Passwords don't match."
                sys.exit(1)
        else:
            options.encrypt = sys.stdin.readline().rstrip()

        # Hash to 16 bytes required for AES
        m = hashlib.md5()
        m.update(options.encrypt)
        options.encrypt = m.digest()
    else:
        options.encrypt = None


    #
    # Setup local data
    #

    dbfile = s3ql.get_dbfile(bucket)
    cachedir = s3ql.get_cachedir(bucket)

    if os.path.exists(dbfile) or \
            os.path.exists(cachedir):
        if options.force:
            if os.path.exists(dbfile):
                os.unlink(dbfile)
            if os.path.exists(cachedir):
                shutil.rmtree(cachedir)
            print "Removed existing metadata."
        else:
            print >> sys.stderr, \
                "Local metadata file already exists!\n" \
                "Use -f option to really remove the existing filesystem\n"
            sys.exit(1)

    #
    # Setup remote data
    #
    conn = S3Connection(awskey, awspass)
    try:
        conn.get_bucket(bucket)
    except S3ResponseError, e:
        if e.status == 404:
            # This is what we want
            pass
        else:
            raise
    else:
        # Bucket exists
        if options.force:
            delete_bucket(conn, bucket)
            print "Removed existing bucket."
        else:
            print >> sys.stderr, \
                "Bucket already exists! \n" \
                "Use -f option to remove the existing bucket.\n"
            sys.exit(1)



    #
    # Create filesystem
    #
    try:
        setup_db(dbfile, options)
        setup_bucket(conn, bucket, dbfile)
    finally:
        # Delete metadata
        os.unlink(dbfile)

    sys.exit(0)


def delete_bucket(conn, bucketname):
    """Removes all keys in the given bucket and the bucket itself.
    """

    print "Removing existing keys..."
    bucket = conn.get_bucket(bucketname)

    for s3key in bucket.list():
        print "\t", s3key.name
        bucket.delete_key(s3key)

    conn.delete_bucket(bucketname)


def setup_bucket(conn, bucketname, dbfile):
    """Creates a bucket and uploads metadata.
    """

    conn.create_bucket(bucketname)
    sleep(5.0) # Give S3 some propagation time
    bucket = conn.get_bucket(bucketname)

    k = Key(bucket)
    k.key = 'metadata'
    k.set_contents_from_filename(dbfile)

    k = Key(bucket)
    k.key = 'dirty'
    k.set_contents_from_string("no")



def setup_db(dbfile,options):
    """Creates the metadata tables
    """

    conn=apsw.Connection(dbfile)
    cursor=conn.cursor()

    # This command creates triggers that ensure referential integrity
    trigger_cmd = Template("""
    CREATE TRIGGER fki_${src_table}_${src_key}
      BEFORE INSERT ON ${src_table}
      FOR EACH ROW BEGIN
          SELECT RAISE(ABORT, 'insert in column $src_key of table $src_table violates foreign key constraint')
          WHERE NEW.$src_key IS NOT NULL AND
                (SELECT $ref_key FROM $ref_table WHERE $ref_key = NEW.$src_key) IS NULL;
      END;

    CREATE TRIGGER fku_${src_table}_${src_key}
      BEFORE UPDATE ON ${src_table}
      FOR EACH ROW BEGIN
          SELECT RAISE(ABORT, 'update in column $src_key of table $src_table violates foreign key constraint')
          WHERE NEW.$src_key IS NOT NULL AND
                (SELECT $ref_key FROM $ref_table WHERE $ref_key = NEW.$src_key) IS NULL;
      END;


    CREATE TRIGGER fkd_${src_table}_${src_key}
      BEFORE DELETE ON $ref_table
      FOR EACH ROW BEGIN
          SELECT RAISE(ABORT, 'delete on table $ref_table violates foreign key constraint of column $src_key in table ${src_table}')
          WHERE (SELECT $src_key FROM $src_table WHERE $src_key = OLD.$ref_key) IS NOT NULL;
      END;
    """)


    # Filesystem parameters
    cursor.execute("""
    CREATE TABLE parameters (
        name        TEXT,
        blocksize   INT NOT NULL,
        last_fsck   INT NOT NULL,
        mountcnt    INT NOT NULL,
        version     INT NOT NULL,
        needs_fsck  BOOLEAN NOT NULL
    );
    INSERT INTO parameters(name,blocksize,last_fsck,mountcnt,needs_fsck,version)
        VALUES(?,?,?,?,?, ?)
    """, (options.name, options.blocksize * 1024, time(), 0, False, 1))

    # Table of filesystem objects
    cursor.execute("""
    CREATE TABLE contents (
        name      BLOB(256) NOT NULL PRIMARY KEY,
        inode     INT NOT NULL REFERENCES inodes(id),
        parent_inode INT NOT NULL REFERENCES inodes(id)
    );
    CREATE INDEX ix_contents_parent_inode ON contents(parent_inode);
    """)

    # Table with filesystem metadata
    # The number of links `refcount` to an inode can in theory
    # be determined from the `contents` table. However, there is no efficient
    # statement to do so (because we have to count `..` references of all
    # subdirectories), so we manage this separately.
    cursor.execute("""
    CREATE TABLE inodes (
        -- id has to specified *exactly* as follows to become
        -- an alias for the rowid
        id        INTEGER PRIMARY KEY,
        uid       INT NOT NULL,
        gid       INT NOT NULL,
        mode      INT NOT NULL,
        mtime     INT NOT NULL,
        atime     INT NOT NULL,
        ctime     INT NOT NULL,
        refcount  INT NOT NULL,

        -- for symlinks only
        target    BLOB,

        -- for files only
        size      INT CHECK (size >= 0),

        -- device nodes
        rdev      INT
    );
    """)
    cursor.execute(trigger_cmd.substitute({ "src_table": "contents",
                                            "src_key": "inode",
                                            "ref_table": "inodes",
                                            "ref_key": "id" }))
    cursor.execute(trigger_cmd.substitute({ "src_table": "contents",
                                            "src_key": "parent_inode",
                                            "ref_table": "inodes",
                                            "ref_key": "id" }))

    # Maps file data chunks to S3 objects
    cursor.execute("""
    CREATE TABLE s3_objects (
        inode     INTEGER NOT NULL REFERENCES inodes(id),
        offset    INT NOT NULL
                  CHECK (offset >= 0),
        s3key     TEXT(30) NOT NULL UNIQUE,
        etag      TEXT,

        -- for caching
        fd        INTEGER,
        atime     INTEGER NOT NULL,
        dirty     BOOLEAN,
        cachefile TEXT UNIQUE
                  CHECK ((fd IS NULL AND cachefile IS NULL)
                         OR (fd IS NOT NULL AND cachefile IS NOT NULL)),

        PRIMARY KEY (inode, offset)
    );
    CREATE INDEX ix_s3 ON s3_objects(s3key);
    CREATE INDEX ix_dirty ON s3_objects(dirty);
    """);
    cursor.execute(trigger_cmd.substitute({ "src_table": "s3_objects",
                                            "src_key": "inode",
                                            "ref_table": "inodes",
                                            "ref_key": "id" }))


    # Create a view of the whole fs with all information
    cursor.execute("""
    CREATE VIEW contents_ext AS
        SELECT * FROM contents JOIN inodes ON id == inode;
    """)


    # Insert root directory
    cursor.execute("INSERT INTO inodes (mode,uid,gid,mtime,atime,ctime,refcount) "
                   "VALUES (?,?,?,?,?,?,?)",
                   (stat.S_IFDIR | stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR
                   | stat.S_IRGRP | stat.S_IXGRP | stat.S_IROTH | stat.S_IXOTH,
                    os.getuid(), os.getgid(), time(), time(), time(), 2))
    inode = conn.last_insert_rowid()
    cursor.execute("INSERT INTO contents (name, inode, parent_inode) VALUES(?,?,?)",
                   (buffer("/"), inode, inode))
    cursor.execute("INSERT INTO contents (name, inode, parent_inode) VALUES(?,?,?)",
                   (buffer("/.."), inode, inode))

    # Done setting up metadata table
    conn.close()



if __name__ == '__main__':
    main()
