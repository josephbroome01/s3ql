#!/usr/bin/env python
#
#    Copyright (C) 2008  Nikolaus Rath <Nikolaus@rath.org>
#
#    This program can be distributed under the terms of the GNU LGPL.
#

# Python boto uses several deprecated modules
import warnings
warnings.filterwarnings("ignore", "", DeprecationWarning, "boto")

from optparse import OptionParser
from getpass  import getpass
import s3ql
import sys
import os
import stat
import apsw
import syslog

#
# Parse command line
#
parser = OptionParser(
    usage="%prog  [options] <bucketname> <mountpoint>\n"
          "       %prog --help",
    description="Mounts an amazon S3 bucket as a filesystem.")

parser.add_option("--awskey", type="string",
                  help="Amazon Webservices access key to use. The password is "
                  "read from stdin. If this option is not specified, both access key "
                  "and password are read from ~/.awssecret (separated by newlines).")
parser.add_option("--debug", action="store_true", default=False,
                  help="Generate debugging output")
parser.add_option("--s3timeout", type="int", default=50,
                  help="Maximum time to wait for propagation in S3 (default: %default)")
parser.add_option("--allow_others", action="store_true", default=False,
                  help="Allow others users to access the filesystem")
parser.add_option("--allow_root", action="store_true", default=False,
                  help="Allow root to access the filesystem")
parser.add_option("--encrypt", action="store_true", default=None,
                  help="Mount an encrypted filesystem")
parser.add_option("--nonempty", action="store_true", default=False,
                  help="Allow mount if even mount point is not empty")
parser.add_option("--fg", action="store_true", default=False,
                  help="Do not daemonize, stay in foreground")
parser.add_option("--cachesize", type="int", default=None,
                  help="Cache size (default: 30 * fs blocksize)")
parser.add_option("--single", action="store_true", default=False,
                  help="Single threaded operation only")

(options, pps) = parser.parse_args()

#
# Verify parameters
#
if not len(pps) == 2:
    parser.error("Wrong number of parameters")
bucketname = pps[0]
mountpoint = pps[1]


#
# Read password(s)
#
(awskey, awspass) = s3ql.get_credentials(options.awskey)

if options.encrypt:
    if sys.stdin.isatty():
        options.encrypt = getpass("Enter encryption password: ")
        if not options.encrypt == getpass("Confirm encryption password: "):
            print >>sys.stderr, "Passwords don't match."
            sys.exit(1)
    else:
        options.encrypt = sys.stdin.readline().rstrip()


#
# Pass on fuse options
#
fuse_opts = []
if options.allow_others:
    fuse_opts.append("allow_others")
if options.allow_root:
    fuse_opts.append("allow_root")
if options.nonempty:
    fuse_opts.append("nonempty")


#
# Activate logging
#
if options.debug:
    s3ql.log_level = 2


#
# Connect to S3
#
conn = s3ql.s3.Connection(awskey, awspass, options.encrypt)
bucket = conn.get_bucket(bucketname)

cachedir = s3ql.get_cachedir(bucketname)
dbfile = s3ql.get_dbfile(bucketname)

#
# Check consistency
#
s3ql.debug("Checking consistency...")
if bucket["s3ql_dirty"] != "no":
    print >> sys.stderr, \
        "Metadata is dirty! Either some changes have not yet propagated\n" \
        "through S3 or the filesystem has not been umounted cleanly. In\n" \
        "the later case you should run s3fsck on the system where the\n" \
        "filesystem has been mounted most recently!\n"
    sys.exit(1)

# Init cache
if os.path.exists(cachedir) or os.path.exists(dbfile):
    print >> sys.stderr, \
        "Local cache files already exists! Either you are trying to\n" \
        "to mount a filesystem that is already mounted, or the filesystem\n" \
        "has not been umounted cleanly. In the later case you should run\n" \
        "s3fsck.\n"
    sys.exit(1)

# Init cache + get metadata
try:
    s3ql.debug("Downloading metadata...")
    os.mknod(dbfile, 0600 | stat.S_IFREG)
    os.mkdir(cachedir, 0700)
    bucket.fetch_to_file("s3ql_metadata", dbfile)

    # Check that the fs itself is clean
    conn = apsw.Connection(dbfile)
    (dirty,) = conn.cursor().execute("SELECT needs_fsck FROM parameters").next()
    conn.close()
    if dirty:
        print >> sys.stderr, "Filesystem damaged, run s3fsk!\n"
        sys.exit(1)

    #
    # Start server
    #
    server = s3ql.fs(bucket, dbfile, cachedir,
                     cachesize=options.cachesize)

    # Init syslog before going bg
    if not options.fg:
        syslog.openlog("s3ql", 0, syslog.LOG_USER)
        def logger(args):
            msg = "".join(args)
            for line in msg.split("\n"):
                if len(line) == 0:
                    continue
                syslog.syslog(line)

        s3ql.log_fn = logger

    s3ql.setup_excepthook(server)
    server.main(mountpoint, fuse_opts, options.fg,
                mt=(not options.single))
    server.close()


    # Upload database
    s3ql.debug("Uploading database..")
    bucket.store_from_file("s3ql_metadata", dbfile)
    bucket.store("s3ql_dirty", "no")

# Remove database
finally:
    # Ignore exceptions when cleaning up
    try:
        s3ql.debug("Cleaning up...")
        os.unlink(dbfile)
        os.rmdir(cachedir)
    except:
        pass
